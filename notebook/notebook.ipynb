{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from apify_client import ApifyClient\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_API_TOKEN = os.getenv('HUGGING_FACE_API_TOKEN')\n",
    "GROQ_API_TOKEN = os.getenv('GROQ_API_TOKEN')\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "# print(HUGGING_FACE_API_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Okay, I'm thinking step by step. The 2010 FIFA World Cup was held in South Africa. I need to recall which team won that year. \\n\\n... Spain won the 2010 FIFA World Cup.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--1984a937-b8e9-42fc-83e2-5915280c63c4-0' usage_metadata={'input_tokens': 29, 'output_tokens': 51, 'total_tokens': 80, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Load LLM Model\n",
    "# from langchain_huggingface import HuggingFaceEndpoint\n",
    "# from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "question = \"Who won the FIFA World Cup in the year 2010? \"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "gemini_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"models/gemini-2.0-flash\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500)\n",
    "\n",
    "# repo_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "# llm = ChatGroq(\n",
    "#     model=\"llama-3.1-8b-instant\",\n",
    "#     temperature=0.0,\n",
    "#     max_retries=2,\n",
    "#     api_key=GROQ_API_TOKEN\n",
    "# )\n",
    "\n",
    "# llama_llm = HuggingFaceEndpoint(\n",
    "#     repo_id=repo_id,\n",
    "#     max_length=500,\n",
    "#     temperature=0.5,\n",
    "#     huggingfacehub_api_token=HUGGING_FACE_API_TOKEN,\n",
    "#     task='text-generation'\n",
    "# )\n",
    "# llm_chain = LLMChain(llm=gemini_llm, prompt=prompt)\n",
    "llm_chain = prompt | gemini_llm\n",
    "print(llm_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def llm_define(llm):\n",
    "#     return llm\n",
    "\n",
    "# lama = llm_define(gemini_llm)\n",
    "\n",
    "# llm_chain = LLMChain(llm=lama, prompt=prompt)\n",
    "# print(llm_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PDF Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dilshan Upasena\\ng.dilshanupasena@gmail.com\\n \\n+94723308678\\n \\nAnuradhapura, Sri Lanka\\n \\nlinkedin.com/in/dilshan-upasena-98gdu\\n \\ngithub.com/G-Dilshan\\n \\nPROFILE\\nComputer Science undergraduate with a strong foundation in Machine Learning, Deep Learning, and\\nData Science. Skilled in developing Al-powered solutions for real-world challenges, with hands-on\\nexperience in computer vision, supervised and unsupervised learning. Familiar with Python and\\nPyTorch, with practical experience in cloud deployment (AWS, Docker), API development using Flask\\nand FastAPI, and database systems including MySQL, MongoDB, and PostgreSQL. Actively exploring\\nGenerative Al, including Retrieval-Augmented Generation (RAG), LangChain, and vector databases such\\nas Pinecone and ChromaDB. Familiar with model deployment and experimentation using platforms like\\nHugging Face and Ollama, and working with large language models such as LLaMA and Gemini. Deeply\\npassionate about academic research and innovative development in Al.\\nEDUCATION\\nBSc Hons in Computer Science\\nUniversity of Jaffna\\n•Current GPA: 3.42/4.00\\nG.C.E Advanced Level (Physical Science Stream)\\nAnuradhapura Central College\\nPROJECTS\\nResearch on Dengue Mosquito Larvae Classification Using Vision Transformers\\n•Designing a Vision Transformer (ViT)-based system to classify Aedes aegypti and Aedes albopictus\\nlarvae from live images, addressing dengue prevention in Sri Lanka.\\n•Comparing transfer learning (pre-trained Swin Transformer/ViT-Base) with custom ViT architectures\\nusing Swin as a backbone, to optimize larval classification accuracy.\\n•The research aims to achieve 92.5%+ accuracy and deploy the model in a mobile application to\\nassist public health inspectors in identifying dengue breeding sites and mapping high-risk areas in\\nSri Lanka.\\nDeep Learning Project: Multi-Region Bone Fracture Classification\\n•Designed a binary classifier (fractured vs. non- fractured) using Swin Transformer (Swin-T) with \\ntransfer learning, addressing challenges in medical image analysis.\\n•Trained on a Kaggle dataset, implementing preprocessing pipelines for images with augmentation \\n(rotation, contrast adjustment) to enhance generalization and deployed a Flask API, enabling real-\\ntime medical image classification.\\nStudent Exam Performance Predictor\\n•A machine learning-based application designed to predict student exam performance based on \\nfactors such as parental education level, test preparation, and previous scores.\\n•Built using CatBoost, XGBoost, and Scikit-Learn for accurate predictions.\\n•Web interface developed with Flask for user-friendly interaction.\\n•Since the data shows a normal distribution, handled missing values using mean imputation, \\nremoved outliers with the Z-score method, and applied StandardScaler for effective standardization.\\nMulti-Agent Career Roadmap Builder (AI-Powered Career Pathway Generator)\\n•Developed an AI-powered career roadmap builder using a multi-agent architecture, orchestrated \\nthrough Langgraph, providing personalized learning paths and career guidance.\\n•Leveraged Gemma2-9b-it LLM to generate research insights, summarize content, and map career \\npaths dynamically.\\n•Technologies: Gemma2-9b-it LLM, Multi-Agent Architecture, Langgraph, Streamlit, Graphviz, Python\\n2021 – 10/2025\\nSri\\xa0Lanka\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\GD\\OneDrive\\Desktop\\Resume\\Dilshan-Upasena.pdf\"\n",
    "\n",
    "# Open the PDF\n",
    "doc = fitz.open(file_path)\n",
    "\n",
    "# Example: print the first page text\n",
    "# print(doc[0].get_text())\n",
    "resume_text = doc[0].get_text()\n",
    "# print(resume_text)\n",
    "doc[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from PDF\n",
    "def extract_text_from_pdf(uploaded_file):\n",
    "    # doc = fitz.open(stream=uploaded_file.read(), filetype='pdf')\n",
    "    doc = fitz.open(file_path)\n",
    "    text = ''\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text = extract_text_from_pdf(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract text from PDF\n",
    "# def extract_text_from_pdf(uploaded_file):\n",
    "#     doc = fitz.open(uploaded_file, filetype='pdf')\n",
    "#     text = ''\n",
    "#     for page in doc:\n",
    "#         text += page.get_text()\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask gemini to generate output\n",
    "def ask_gemini(prompt, resume_text):\n",
    "    llm_chain = prompt | gemini_llm\n",
    "    response = llm_chain.invoke(resume_text)\n",
    "    if isinstance(response, dict) and 'choices' in response:\n",
    "        return response['choices'][0]['message']['content']\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_summary = PromptTemplate(\n",
    "    input_variables=[\"resume_text\"],\n",
    "    template=\"Summarize this resume highlighting skills, education, and experience:\\n\\n{resume_text}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = ask_gemini(prompt=prompt_summary, resume_text=resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a summary of Dilshan Upasena's resume:\\n\\n**Summary:**\\n\\nDilshan Upasena is a Computer Science undergraduate with a strong foundation and passion for Machine Learning, Deep Learning, and Data Science, particularly in Generative AI. He possesses hands-on experience in developing AI-powered solutions using Python, PyTorch, and various frameworks for tasks like computer vision, classification, and prediction. He's familiar with cloud deployment, API development, and database management, and is actively exploring cutting-edge technologies like Retrieval-Augmented Generation (RAG), LangChain, and large language models.\\n\\n**Skills:**\\n\\n*   **Machine Learning/Data Science:** Supervised/Unsupervised Learning, Computer Vision, Data Visualization, ANN, CNN\\n*   **Generative AI & NLP:** LangChain, Hugging Face, Ollama, RAG, LLAMA, Gemini\\n*   **Programming Languages:** Python, Java\\n*   **Python Packages/Frameworks:** Pytorch, NumPy, Pandas, Scikit-Learn, Matplotlib, Pillow\\n*   **Vector Databases:** Pinecone, ChromaDB\\n*   **Databases:** MySQL, MongoDB, PostgreSQL\\n*   **API Development:** Flask, FastAPI\\n*   **Cloud Deployment/Containers:** AWS EC2, Docker\\n*   **Version Control/Tools:** Git, GitHub, Postman\\n\\n**Education:**\\n\\n*   BSc Hons in Computer Science, University of Jaffna (Expected 2025), GPA: 3.42/4.00\\n*   G.C.E Advanced Level (Physical Science Stream), Anuradhapura Central College\\n\\n**Projects:**\\n\\n*   **Research on Dengue Mosquito Larvae Classification:** Developed a Vision Transformer (ViT)-based system to classify mosquito larvae using live images, aiming for 92.5%+ accuracy and deployment in a mobile app for public health inspectors.\\n*   **Multi-Region Bone Fracture Classification:** Designed a binary classifier using Swin Transformer (Swin-T) with transfer learning, deployed as a Flask API for real-time medical image classification.\\n*   **Student Exam Performance Predictor:** A machine learning application using CatBoost, XGBoost, and Scikit-Learn to predict student exam performance, with a Flask web interface.\\n*   **Multi-Agent Career Roadmap Builder (AI-Powered Career Pathway Generator):** Developed an AI-powered career\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Keywords for Job Search from linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_roadmap = PromptTemplate(\n",
    "        input_variables=[\"summary\"],  \n",
    "        template=\"Based on this resume summary, suggest the best job titles/keywords for searching jobs. Give a comma-separated list only, no explanation.\\n\\nSummary:\\n{summary}\"\n",
    "        )\n",
    "keywords = ask_gemini(prompt=prompt_roadmap, resume_text={\"summary\": summary.content})\n",
    "search_keywords_clean = keywords.content.replace(\"\\n\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine Learning Engineer, Data Scientist, AI Engineer, Generative AI Engineer, Deep Learning Engineer, Computer Vision Engineer, NLP Engineer, AI Developer, Machine Learning Developer, Data Science Intern, AI Intern, Python Developer, Pytorch Engineer, RAG Engineer, LangChain Developer, LLM Engineer, AI Research Assistant'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_keywords_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Apify linkedin Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Apify api\n",
    "APIFY_API_TOKEN = os.getenv('APIFY_API_TOKEN')\n",
    "apify_client = ApifyClient(token=APIFY_API_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Linkedin jobs using Apify\n",
    "def fetch_linkedin_jobs(search_query, location=\"Sri Lanka\", rows=60):\n",
    "    run_input = {\n",
    "        'title': search_query,\n",
    "        'location': location,\n",
    "        'rows': rows,\n",
    "        'proxy': {\n",
    "            'useApifyProxy': True,\n",
    "            'apifyProxyGroups': ['RESIDENTIAL']\n",
    "        }\n",
    "    }\n",
    "    run = apify_client.actor('api_key_for_apify').call(run_input=run_input)\n",
    "    jobs = list(apify_client.dataset(run['defaultDatasetId']).iterate_items())\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ApifyApiError",
     "evalue": "Authentication token was not provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mApifyApiError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m linkedin_results \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_linkedin_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_keywords_clean\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[65], line 12\u001b[0m, in \u001b[0;36mfetch_linkedin_jobs\u001b[1;34m(search_query, location, rows)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfetch_linkedin_jobs\u001b[39m(search_query, location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSri Lanka\u001b[39m\u001b[38;5;124m\"\u001b[39m, rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m):\n\u001b[0;32m      3\u001b[0m     run_input \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: search_query,\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m'\u001b[39m: location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m         }\n\u001b[0;32m     11\u001b[0m     }\n\u001b[1;32m---> 12\u001b[0m     run \u001b[38;5;241m=\u001b[39m \u001b[43mapify_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mapi_key_for_apify\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(apify_client\u001b[38;5;241m.\u001b[39mdataset(run[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefaultDatasetId\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39miterate_items())\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jobs\n",
      "File \u001b[1;32mc:\\Users\\GD\\venvs\\envAI\\lib\\site-packages\\apify_client\\_logging.py:82\u001b[0m, in \u001b[0;36m_injects_client_details_to_log_context.<locals>.wrapper\u001b[1;34m(resource_client, *args, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m log_context\u001b[38;5;241m.\u001b[39mclient_method\u001b[38;5;241m.\u001b[39mset(fun\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m)\n\u001b[0;32m     80\u001b[0m log_context\u001b[38;5;241m.\u001b[39mresource_id\u001b[38;5;241m.\u001b[39mset(resource_client\u001b[38;5;241m.\u001b[39mresource_id)\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fun(resource_client, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\GD\\venvs\\envAI\\lib\\site-packages\\apify_client\\clients\\resource_clients\\actor.py:320\u001b[0m, in \u001b[0;36mActorClient.call\u001b[1;34m(self, run_input, content_type, build, max_items, max_total_charge_usd, memory_mbytes, timeout_secs, webhooks, wait_secs)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m     wait_secs: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    292\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    293\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Start the Actor and wait for it to finish before returning the Run object.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03m    It waits indefinitely, unless the wait_secs argument is provided.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;124;03m        The run object.\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 320\u001b[0m     started_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuild\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_total_charge_usd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_total_charge_usd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_mbytes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mbytes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout_secs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_secs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwebhooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwebhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mrun(started_run[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mwait_for_finish(wait_secs\u001b[38;5;241m=\u001b[39mwait_secs)\n",
      "File \u001b[1;32mc:\\Users\\GD\\venvs\\envAI\\lib\\site-packages\\apify_client\\_logging.py:82\u001b[0m, in \u001b[0;36m_injects_client_details_to_log_context.<locals>.wrapper\u001b[1;34m(resource_client, *args, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m log_context\u001b[38;5;241m.\u001b[39mclient_method\u001b[38;5;241m.\u001b[39mset(fun\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m)\n\u001b[0;32m     80\u001b[0m log_context\u001b[38;5;241m.\u001b[39mresource_id\u001b[38;5;241m.\u001b[39mset(resource_client\u001b[38;5;241m.\u001b[39mresource_id)\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fun(resource_client, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\GD\\venvs\\envAI\\lib\\site-packages\\apify_client\\clients\\resource_clients\\actor.py:270\u001b[0m, in \u001b[0;36mActorClient.start\u001b[1;34m(self, run_input, content_type, build, max_items, max_total_charge_usd, memory_mbytes, timeout_secs, wait_for_finish, webhooks)\u001b[0m\n\u001b[0;32m    258\u001b[0m run_input, content_type \u001b[38;5;241m=\u001b[39m encode_key_value_store_record_value(run_input, content_type)\n\u001b[0;32m    260\u001b[0m request_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params(\n\u001b[0;32m    261\u001b[0m     build\u001b[38;5;241m=\u001b[39mbuild,\n\u001b[0;32m    262\u001b[0m     maxItems\u001b[38;5;241m=\u001b[39mmax_items,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m     webhooks\u001b[38;5;241m=\u001b[39mencode_webhook_list_to_base64(webhooks) \u001b[38;5;28;01mif\u001b[39;00m webhooks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    268\u001b[0m )\n\u001b[1;32m--> 270\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mruns\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent-type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parse_date_fields(pluck_data(response\u001b[38;5;241m.\u001b[39mjson()))\n",
      "File \u001b[1;32mc:\\Users\\GD\\venvs\\envAI\\lib\\site-packages\\apify_client\\_http_client.py:218\u001b[0m, in \u001b[0;36mHTTPClient.call\u001b[1;34m(self, method, url, headers, params, data, json, stream, parse_response, timeout_secs)\u001b[0m\n\u001b[0;32m    215\u001b[0m         stop_retrying()\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ApifyApiError(response, attempt)\n\u001b[1;32m--> 218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_with_exp_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackoff_base_millis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_delay_between_retries_millis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackoff_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEFAULT_BACKOFF_EXPONENTIAL_FACTOR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEFAULT_BACKOFF_RANDOM_FACTOR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GD\\venvs\\envAI\\lib\\site-packages\\apify_client\\_utils.py:66\u001b[0m, in \u001b[0;36mretry_with_exp_backoff\u001b[1;34m(func, max_retries, backoff_base_millis, backoff_factor, random_factor)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_retries \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstop_retrying\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m swallow:\n",
      "File \u001b[1;32mc:\\Users\\GD\\venvs\\envAI\\lib\\site-packages\\apify_client\\_http_client.py:216\u001b[0m, in \u001b[0;36mHTTPClient.call.<locals>._make_request\u001b[1;34m(stop_retrying, attempt)\u001b[0m\n\u001b[0;32m    214\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus code is not retryable\u001b[39m\u001b[38;5;124m'\u001b[39m, extra\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus_code\u001b[39m\u001b[38;5;124m'\u001b[39m: response\u001b[38;5;241m.\u001b[39mstatus_code})\n\u001b[0;32m    215\u001b[0m     stop_retrying()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ApifyApiError(response, attempt)\n",
      "\u001b[1;31mApifyApiError\u001b[0m: Authentication token was not provided"
     ]
    }
   ],
   "source": [
    "linkedin_results = fetch_linkedin_jobs(search_query=search_keywords_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Naukri jobs using Apify\n",
    "def fetch_naukri_jobs(search_query, location=\"Sri Lanka\", rows=60):\n",
    "    run_input = {\n",
    "        'keyword': search_query,\n",
    "        'maxJobs': 60,\n",
    "        'freshness': 'all',\n",
    "        'sortBy': 'relevance',\n",
    "        'experience': 'all'\n",
    "    }\n",
    "    run = apify_client.actor('api_key_for_apify').call(run_input=run_input)\n",
    "    jobs = list(apify_client.dataset(run['defaultDatasetId']).iterate_items())\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "umUj9ttmMtFzlhTa3\n",
      "alpcnRV9YI9lYVPWk\n"
     ]
    }
   ],
   "source": [
    "LINKEDIN_APIFY_ACTOR = os.getenv('LINKEDIN_APIFY_ACTOR')\n",
    "NAUKRI_APIFY_ACTOR = os.getenv('NAUKRI_APIFY_ACTOR')\n",
    "\n",
    "print(LINKEDIN_APIFY_ACTOR)\n",
    "print(NAUKRI_APIFY_ACTOR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
