{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from apify_client import ApifyClient\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_API_TOKEN = os.getenv('HUGGING_FACE_API_TOKEN')\n",
    "GROQ_API_TOKEN = os.getenv('GROQ_API_TOKEN')\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "# print(HUGGING_FACE_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected argument 'task' provided to GoogleGenerativeAI. Did you mean: 'tags'?\n",
      "Unexpected argument 'max_length' provided to GoogleGenerativeAI. Did you mean: 'max_tokens'?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Who won the FIFA World Cup in the year 2010? ', 'text': \"Okay, I'm thinking step by step... The 2010 FIFA World Cup was held in South Africa. I remember the final was between Spain and the Netherlands. And... Spain won! \\n\\nTherefore, the answer is **Spain**.\"}\n"
     ]
    }
   ],
   "source": [
    "# Load LLM Model\n",
    "# from langchain_huggingface import HuggingFaceEndpoint\n",
    "# from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "question = \"Who won the FIFA World Cup in the year 2010? \"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "gemini_llm = GoogleGenerativeAI(\n",
    "    model=\"models/gemini-2.0-flash\",\n",
    "    temperature=0.5,\n",
    "    task='text_generation',\n",
    "    max_length=500)\n",
    "\n",
    "# repo_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "# llm = ChatGroq(\n",
    "#     model=\"llama-3.1-8b-instant\",\n",
    "#     temperature=0.0,\n",
    "#     max_retries=2,\n",
    "#     api_key=GROQ_API_TOKEN\n",
    "# )\n",
    "\n",
    "# llama_llm = HuggingFaceEndpoint(\n",
    "#     repo_id=repo_id,\n",
    "#     max_length=500,\n",
    "#     temperature=0.5,\n",
    "#     huggingfacehub_api_token=HUGGING_FACE_API_TOKEN,\n",
    "#     task='text-generation'\n",
    "# )\n",
    "llm_chain = LLMChain(llm=gemini_llm, prompt=prompt)\n",
    "print(llm_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Who won the FIFA World Cup in the year 2010? ', 'text': \"Okay, let's think step by step. The 2010 FIFA World Cup was held in South Africa. I remember that Spain played very well in that tournament. \\n\\nTherefore, the answer is **Spain**.\"}\n"
     ]
    }
   ],
   "source": [
    "def llm_define(llm):\n",
    "    return llm\n",
    "\n",
    "lama = llm_define(gemini_llm)\n",
    "\n",
    "llm_chain = LLMChain(llm=lama, prompt=prompt)\n",
    "print(llm_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Apify api\n",
    "APIFY_API_TOKEN = os.getenv('APIFY_API_TOKEN')\n",
    "apify_client = ApifyClient(token=APIFY_API_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from PDF\n",
    "def extract_text_from_pdf(uploaded_file):\n",
    "    doc = fitz.open(stream=uploaded_file.read(), filetype='pdf')\n",
    "    text = ''\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from PDF\n",
    "def extract_text_from_pdf(uploaded_file):\n",
    "    doc = fitz.open(uploaded_file, filetype='pdf')\n",
    "    text = ''\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask LLAMA to generate output\n",
    "def ask_gemini(prompt, resume_text):\n",
    "    llm_chain = LLMChain(llm=gemini_llm, prompt=prompt)\n",
    "    response = llm_chain.invoke(resume_text)\n",
    "    # response = llama_client.generate_completion(prompt=prompt, temperature=0.5)\n",
    "    if isinstance(response, dict) and 'choices' in response:\n",
    "    # if isinstance(response) and 'choices' in response:\n",
    "        return response['choices'][0]['message']['content']\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Linkedin jobs using Apify\n",
    "def fetch_linkedin_jobs(search_query, location=\"Sri Lanka\", rows=60):\n",
    "    run_input = {\n",
    "        'title': search_query,\n",
    "        'location': location,\n",
    "        'rows': rows,\n",
    "        'proxy': {\n",
    "            'useApifyProxy': True,\n",
    "            'apifyProxyGroups': ['RESIDENTIAL']\n",
    "        }\n",
    "    }\n",
    "    run = apify_client.actor('api_key_for_apify').call(run_input=run_input)\n",
    "    jobs = list(apify_client.dataset(run['defaultDatasetId']).iterate_items())\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Naukri jobs using Apify\n",
    "def fetch_naukri_jobs(search_query, location=\"Sri Lanka\", rows=60):\n",
    "    run_input = {\n",
    "        'keyword': search_query,\n",
    "        'maxJobs': 60,\n",
    "        'freshness': 'all',\n",
    "        'sortBy': 'relevance',\n",
    "        'experience': 'all'\n",
    "    }\n",
    "    run = apify_client.actor('api_key_for_apify').call(run_input=run_input)\n",
    "    jobs = list(apify_client.dataset(run['defaultDatasetId']).iterate_items())\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'+94 72 330 8678\\nAnuradhapura, Sri Lanka\\ng.dilshanupasena@gmail.com\\nwww.linkedin.com/in/dilshan-upasena-98gdu\\nDILSHAN UPASENA\\nPROFESSIONAL SUMMARY\\nComputer Science undergraduate with a strong foundation in Machine Learning, Deep Learning, and Data Science. Skilled\\nin developing AI-powered solutions for real-world challenges, with hands-on experience in computer vision, supervised and\\nunsupervised learning. Familiar with Python and PyTorch, with practical experience in cloud deployment (AWS, Docker),\\nAPI development using Flask and FastAPI, and database systems including MySQL, MongoDB, and PostgreSQL. Actively\\nexploring Generative AI, including Retrieval-Augmented Generation (RAG), LangChain, and vector databases such as\\nPinecone and ChromaDB. Familiar with model deployment and experimentation using platforms like Hugging Face and\\nOllama, and working with large language models such as LLaMA and Gemini. Deeply passionate about academic research\\nand innovative development in AI.\\nEDUCATION\\nPROJECTS\\nCurrent GPA: 3.42/4.00\\nUniversity of Jaffna, Sri Lanka\\nBSc Hons in Computer Science | 2021 - Oct 2025\\nResearch on Dengue Mosquito Larvae\\nClassification Using Vision Transformers\\n(2024-Current)\\nDesigning a Vision Transformer (ViT)-based system\\nto classify Aedes aegypti and Aedes albopictus\\nlarvae \\nfrom \\nlive \\nimages, \\naddressing \\ndengue\\nprevention in Sri Lanka.\\nhttps://github.com/G-Dilshan\\nComparing \\ntransfer \\nlearning \\n(pre-trained \\nSwin\\nTransformer/ViT-Base) with custom ViT architectures\\nusing Swin as a backbone, to optimize larval\\nclassification accuracy.\\nDeep Learning Project: Multi-Region Bone\\nFracture Classification (2025)\\nDesigned a binary classifier (fractured vs. non-\\nfractured) using Swin Transformer (Swin-T) with\\ntransfer learning, addressing challenges in medical\\nimage analysis.\\nTrained \\non \\na \\nKaggle \\ndataset, \\nimplementing\\npreprocessing \\npipelines \\nfor \\nimages \\nwith\\naugmentation (rotation, contrast adjustment) to\\nenhance generalization and deployed a Flask API,\\nenabling real-time medical image classification.\\nThe research aims to achieve 92.5%+ accuracy and\\ndeploy the model in a mobile application to assist\\npublic health inspectors in identifying dengue\\nbreeding sites and mapping high-risk areas in Sri\\nLanka.\\nAnuradhapura Central College\\nG.C.E Advanced Level (Physical Science Stream)\\nStudent Exam Performance Predictor (2025)\\nA machine learning-based application designed to\\npredict student exam performance based on factors\\nsuch as parental education level, test preparation, and\\nprevious scores.\\nBuilt using CatBoost, XGBoost, and Scikit-Learn for\\naccurate predictions.\\nWeb interface developed with Flask for user-friendly\\ninteraction.\\nSince the data shows a normal distribution, handled\\nmissing values using mean imputation, removed outliers\\nwith the Z-score method, and applied StandardScaler for\\neffective standardization.\\nChat with Multiple PDFs using Gemini Pro &\\nFAISS (2025)\\nDeveloped \\na \\ndocument \\nQ&A \\napplication \\nusing\\nGenerative AI and FAISS.\\nBuilt a semantic search system to process multiple\\nPDFs, extract text, and generate embeddings using\\nGoogle Generative AI.\\nTechnologies: Python, Streamlit, LangChain, Gemini,\\nFAISS.\\nWeb App for MATRIX Online Institute - Team\\nProject (2024)\\n3rd-year \\nuniversity \\nproject, \\ndeveloped \\nan \\nweb\\napplication for MATRIX online institute(an educational\\ninstitute).\\nRole: Team Lead and Backend Developer\\n       Technologies: ExpressJS, PostgreSQL\\nArtificial Intelligence / Machine Learning / Data Science\\n'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing\n",
    "# Use raw string to avoid escaping backslashes\n",
    "file_path = r\"C:\\Users\\GD\\Desktop\\Resume\\Dilshan Upasena.pdf\"\n",
    "\n",
    "# Open the PDF\n",
    "doc = fitz.open(file_path)\n",
    "\n",
    "# Example: print the first page text\n",
    "# print(doc[0].get_text())\n",
    "resume_text = doc[0].get_text()\n",
    "# print(resume_text)\n",
    "doc[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text = extract_text_from_pdf(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_summary = PromptTemplate(\n",
    "    input_variables=[\"resume_text\"],  # this is the key you will use later\n",
    "    template=\"Summarize this resume highlighting skills, education, and experience:\\n\\n{resume_text}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = ask_gemini(prompt=prompt_summary, resume_text=resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'resume_text': '+94 72 330 8678\\nAnuradhapura, Sri Lanka\\ng.dilshanupasena@gmail.com\\nwww.linkedin.com/in/dilshan-upasena-98gdu\\nDILSHAN UPASENA\\nPROFESSIONAL SUMMARY\\nComputer Science undergraduate with a strong foundation in Machine Learning, Deep Learning, and Data Science. Skilled\\nin developing AI-powered solutions for real-world challenges, with hands-on experience in computer vision, supervised and\\nunsupervised learning. Familiar with Python and PyTorch, with practical experience in cloud deployment (AWS, Docker),\\nAPI development using Flask and FastAPI, and database systems including MySQL, MongoDB, and PostgreSQL. Actively\\nexploring Generative AI, including Retrieval-Augmented Generation (RAG), LangChain, and vector databases such as\\nPinecone and ChromaDB. Familiar with model deployment and experimentation using platforms like Hugging Face and\\nOllama, and working with large language models such as LLaMA and Gemini. Deeply passionate about academic research\\nand innovative development in AI.\\nEDUCATION\\nPROJECTS\\nCurrent GPA: 3.42/4.00\\nUniversity of Jaffna, Sri Lanka\\nBSc Hons in Computer Science | 2021 - Oct 2025\\nResearch on Dengue Mosquito Larvae\\nClassification Using Vision Transformers\\n(2024-Current)\\nDesigning a Vision Transformer (ViT)-based system\\nto classify Aedes aegypti and Aedes albopictus\\nlarvae \\nfrom \\nlive \\nimages, \\naddressing \\ndengue\\nprevention in Sri Lanka.\\nhttps://github.com/G-Dilshan\\nComparing \\ntransfer \\nlearning \\n(pre-trained \\nSwin\\nTransformer/ViT-Base) with custom ViT architectures\\nusing Swin as a backbone, to optimize larval\\nclassification accuracy.\\nDeep Learning Project: Multi-Region Bone\\nFracture Classification (2025)\\nDesigned a binary classifier (fractured vs. non-\\nfractured) using Swin Transformer (Swin-T) with\\ntransfer learning, addressing challenges in medical\\nimage analysis.\\nTrained \\non \\na \\nKaggle \\ndataset, \\nimplementing\\npreprocessing \\npipelines \\nfor \\nimages \\nwith\\naugmentation (rotation, contrast adjustment) to\\nenhance generalization and deployed a Flask API,\\nenabling real-time medical image classification.\\nThe research aims to achieve 92.5%+ accuracy and\\ndeploy the model in a mobile application to assist\\npublic health inspectors in identifying dengue\\nbreeding sites and mapping high-risk areas in Sri\\nLanka.\\nAnuradhapura Central College\\nG.C.E Advanced Level (Physical Science Stream)\\nStudent Exam Performance Predictor (2025)\\nA machine learning-based application designed to\\npredict student exam performance based on factors\\nsuch as parental education level, test preparation, and\\nprevious scores.\\nBuilt using CatBoost, XGBoost, and Scikit-Learn for\\naccurate predictions.\\nWeb interface developed with Flask for user-friendly\\ninteraction.\\nSince the data shows a normal distribution, handled\\nmissing values using mean imputation, removed outliers\\nwith the Z-score method, and applied StandardScaler for\\neffective standardization.\\nChat with Multiple PDFs using Gemini Pro &\\nFAISS (2025)\\nDeveloped \\na \\ndocument \\nQ&A \\napplication \\nusing\\nGenerative AI and FAISS.\\nBuilt a semantic search system to process multiple\\nPDFs, extract text, and generate embeddings using\\nGoogle Generative AI.\\nTechnologies: Python, Streamlit, LangChain, Gemini,\\nFAISS.\\nWeb App for MATRIX Online Institute - Team\\nProject (2024)\\n3rd-year \\nuniversity \\nproject, \\ndeveloped \\nan \\nweb\\napplication for MATRIX online institute(an educational\\ninstitute).\\nRole: Team Lead and Backend Developer\\n       Technologies: ExpressJS, PostgreSQL\\nArtificial Intelligence / Machine Learning / Data Science\\nSchool Volleyball Team - Captain\\nAll Island Volleyball C.W.W. Kannangara Memorial\\nTrophy(2014) - Second Runner-up\\nAll Island School Volleyball Tournament (Participant)\\nUnder-17 Provincial Champions\\n       (North Central Province - 2014)\\nBest Volleyball Player of the School\\n         * Under-17 (2014)\\n         * Under-19 (2016)\\nEXTRACURRICULAR\\nNeural Networks and Deep Learning\\nDeepLearning.AI -  Jan 2025\\nCERTIFICATIONS\\nMachine Learning with Python\\nCoursera -  May 2024\\nUsing Python to Interact with the Operating\\nSystem\\nGoogle -  Apr 2024\\nCrash Course on Python\\nGoogle -  Mar 2024\\nedX Verified Certificate for Product Design,\\nPrototyping, and Testing\\nedX -  Dec 2023\\nMachine Learning / Data Science\\nSupervised Learning, Unsupervised Learning,\\nComputer Vision, Data Visualization, ANN, CNN\\nSKILLS\\nProgramming Languages\\nPython, Java\\nPython Packages and Frameworks\\nPytorch, NumPy, Pandas, Scikit-Learn, Matplotlib,\\nPillow\\nDatabases\\nMySQL, MongoDB, PostgreSQL\\nAPI Development\\nFlask, FastAPI\\nCloud Deployment and Containers\\nAWS EC2, Docker\\nMr. S. Suthakar\\nSenior Lecturer Gr.I  |  Former Head of the Department (2020-2023)\\nREFERENCES\\nDepartment of Computer Science\\nUniversity of Jaffna\\nsosuthakar@univ.jfn.ac.lk\\nVersion Control & Tools\\nGit, GitHub, Postman\\nGenerative AI & NLP\\nLangChain, Hugging Face, Ollama, RAG, LLaMA,\\nGemini\\nVector Databases\\nPinecone, ChromaDB\\n', 'text': \"Dilshan Upasena is a Computer Science undergraduate with a strong foundation in Machine Learning, Deep Learning, and Data Science. He possesses skills in Python and PyTorch, cloud deployment (AWS, Docker), API development (Flask, FastAPI), and database systems (MySQL, MongoDB, PostgreSQL). He's actively exploring Generative AI, including RAG, LangChain, and vector databases.\\n\\n**Education:**\\n\\n*   BSc Hons in Computer Science, University of Jaffna (2021 - Oct 2025), Current GPA: 3.42/4.00\\n*   G.C.E Advanced Level (Physical Science Stream), Anuradhapura Central College\\n\\n**Key Skills:**\\n\\n*   **AI/ML/Data Science:** Supervised/Unsupervised Learning, Computer Vision, Data Visualization, ANN, CNN, Generative AI, NLP, RAG, LangChain, LLaMA, Gemini.\\n*   **Programming:** Python, Java\\n*   **Frameworks/Libraries:** Pytorch, NumPy, Pandas, Scikit-Learn, Matplotlib, Pillow\\n*   **Databases:** MySQL, MongoDB, PostgreSQL\\n*   **API Development:** Flask, FastAPI\\n*   **Cloud/Containers:** AWS EC2, Docker\\n*   **Version Control/Tools:** Git, GitHub, Postman\\n*   **Vector Databases:** Pinecone, ChromaDB\\n\\n**Experience (Projects):**\\n\\n*   **Research on Dengue Mosquito Larvae Classification Using Vision Transformers (2024-Current):** Designing a ViT-based system to classify mosquito larvae, aiming for 92.5%+ accuracy and mobile app deployment.\\n*   **Deep Learning Project: Multi-Region Bone Fracture Classification (2025):** Developed a binary classifier using Swin Transformer with transfer learning and deployed a Flask API for real-time medical image classification.\\n*   **Student Exam Performance Predictor (2025):** Built a machine learning application using CatBoost, XGBoost, and Scikit-Learn, with a Flask web interface.\\n*   **Chat with Multiple PDFs using Gemini Pro & FAISS (2025):** Developed a document Q&A application using Generative AI and FAISS with Python, Streamlit, LangChain, and Gemini.\\n*   **Web App for MATRIX Online Institute (2024):** Team Lead and Backend Developer for a web application using ExpressJS and PostgreSQL.\\n\\n**Certifications:**\\n\\n*   Neural Networks and Deep Learning (DeepLearning.AI)\\n*   Machine Learning with Python (Coursera)\\n*   Using Python to Interact with the Operating System (Google)\\n*   Crash Course on Python (Google)\\n*   Product Design, Prototyping, and Testing (edX)\"}\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "umUj9ttmMtFzlhTa3\n",
      "alpcnRV9YI9lYVPWk\n"
     ]
    }
   ],
   "source": [
    "LINKEDIN_APIFY_ACTOR = os.getenv('LINKEDIN_APIFY_ACTOR')\n",
    "NAUKRI_APIFY_ACTOR = os.getenv('NAUKRI_APIFY_ACTOR')\n",
    "\n",
    "print(LINKEDIN_APIFY_ACTOR)\n",
    "print(NAUKRI_APIFY_ACTOR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (JobFinder)",
   "language": "python",
   "name": "jobfinder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
